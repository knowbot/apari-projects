{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGzC3uqmuKZB"
   },
   "source": [
    "# In this project we will solve two simple environments using a Q-table and a Neural Network (Deep Q-learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYeKUsX8uXSF"
   },
   "source": [
    "# Subproject 1\n",
    "\n",
    "Solve [`FrozenLake8x8-v0`](https://gym.openai.com/envs/FrozenLake8x8-v0/) using a Q-table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGAOGNSWyncb"
   },
   "source": [
    "1. Import Necessary Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7KHXZDxys6J"
   },
   "source": [
    "\n",
    "2. Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake8x8-v0\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMs2BVFZywAJ"
   },
   "source": [
    "3. Set up the QTable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions:  4\n",
      "States:  64\n",
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "print(\"Actions: \", action_size)\n",
    "state_size = env.observation_space.n\n",
    "print(\"States: \", state_size)\n",
    "obs = env.reset()\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHuDteJVy2_C"
   },
   "source": [
    "4. The Q-Learning algorithm training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "tot_eps = 50000\n",
    "tot_test_eps = 20\n",
    "\n",
    "lr = 0.01\n",
    "discount = 0.97\n",
    "\n",
    "epsilon = 1\n",
    "epsilon_max = 0.9\n",
    "epsilon_min = 0.01\n",
    "decay = 0.01\n",
    "\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "print(qtable.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "[[1.57371960e-03 1.62248291e-03 1.63482394e-03 1.63752411e-03]\n",
      " [1.66163805e-03 1.75584088e-03 1.82161302e-03 1.81161464e-03]\n",
      " [1.88142269e-03 2.04875025e-03 2.12988025e-03 2.08283855e-03]\n",
      " [2.23532919e-03 2.40802092e-03 2.60767832e-03 2.60944105e-03]\n",
      " [2.80624345e-03 3.06907531e-03 3.27801953e-03 3.13699770e-03]\n",
      " [3.63935849e-03 3.91919426e-03 4.23649108e-03 3.85392075e-03]\n",
      " [4.75905626e-03 4.92472740e-03 5.07369881e-03 4.44967203e-03]\n",
      " [5.67761888e-03 5.70032515e-03 5.59628301e-03 4.85035646e-03]\n",
      " [1.48324753e-03 1.50460534e-03 1.52135774e-03 1.58274825e-03]\n",
      " [1.52538233e-03 1.56858722e-03 1.64000092e-03 1.68695752e-03]\n",
      " [1.71478665e-03 1.70437365e-03 1.83882340e-03 1.94041130e-03]\n",
      " [1.33614231e-03 1.68848779e-03 1.90894413e-03 2.35652554e-03]\n",
      " [2.46183821e-03 2.76545263e-03 3.08062416e-03 3.02586623e-03]\n",
      " [3.40283975e-03 4.00722968e-03 4.39379026e-03 4.19848733e-03]\n",
      " [5.27545937e-03 6.30889227e-03 6.29613691e-03 5.17394834e-03]\n",
      " [7.00221673e-03 8.00516723e-03 7.55925410e-03 5.74683306e-03]\n",
      " [1.34700952e-03 1.25629953e-03 1.31806541e-03 1.41513149e-03]\n",
      " [1.33656841e-03 1.22265285e-03 1.36736808e-03 1.41742681e-03]\n",
      " [1.35352523e-03 7.78910950e-04 9.16126034e-04 1.09881650e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.03112556e-03 1.42111366e-03 2.22370362e-03 2.12465260e-03]\n",
      " [1.90463204e-03 2.85530390e-03 3.41772125e-03 4.04180304e-03]\n",
      " [5.74453365e-03 7.43220776e-03 8.32701209e-03 5.97786892e-03]\n",
      " [1.06219217e-02 1.22857090e-02 1.18133089e-02 8.06938892e-03]\n",
      " [1.08668544e-03 9.59237528e-04 1.05143816e-03 1.15975973e-03]\n",
      " [1.04008906e-03 8.71836759e-04 9.05572053e-04 1.10012860e-03]\n",
      " [8.50032189e-04 6.15298447e-04 6.93363754e-04 8.94777994e-04]\n",
      " [1.93918806e-04 4.08735309e-04 1.99023724e-04 4.52116394e-04]\n",
      " [7.37923389e-04 2.15123083e-04 5.79325869e-04 6.07075997e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.40869743e-03 9.23571325e-03 1.00585886e-02 7.36202936e-03]\n",
      " [1.88423049e-02 2.04449038e-02 2.04417896e-02 1.22287869e-02]\n",
      " [7.06632453e-04 5.71681276e-04 6.61673854e-04 8.38515131e-04]\n",
      " [5.69979360e-04 4.21124346e-04 4.78945375e-04 7.11649491e-04]\n",
      " [4.64349655e-04 1.95403152e-04 2.77674599e-04 4.26996898e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.15929340e-04 3.08234397e-04 5.03406037e-04 4.66306804e-04]\n",
      " [6.58877961e-05 1.36135352e-03 1.90341493e-03 2.01570676e-03]\n",
      " [2.24664448e-03 8.91430109e-03 1.03296962e-02 1.31990961e-02]\n",
      " [3.24034951e-02 4.30360990e-02 4.37164245e-02 1.68848422e-02]\n",
      " [3.51892832e-04 1.26297231e-04 2.80284182e-04 3.29465865e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.49968154e-10 1.29826120e-06 1.26484503e-06 6.73841619e-07]\n",
      " [2.20645451e-05 7.56653176e-06 3.65152137e-05 3.68934009e-05]\n",
      " [9.88744244e-05 3.84290320e-06 1.26255954e-04 6.83769308e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [7.46478274e-02 1.02084108e-01 9.20119742e-02 2.87575288e-02]\n",
      " [1.19754387e-04 2.83121633e-05 7.71947877e-05 9.92005832e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.03310204e-09 1.27507568e-09 3.85888226e-09 4.46114013e-10]\n",
      " [1.21582940e-08 1.53091958e-11 1.39633671e-08 9.99958690e-09]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.50044463e-05 1.58923010e-05 2.13200729e-05 1.95293803e-06]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.49458240e-01 2.72995679e-01 2.83787304e-01 4.98649584e-02]\n",
      " [3.23487012e-05 1.22256241e-05 2.40625602e-05 2.52430475e-05]\n",
      " [3.55295982e-06 3.15960295e-06 4.20751263e-07 4.45242143e-06]\n",
      " [1.37257745e-07 1.65300111e-07 7.91455032e-09 1.54420869e-07]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [9.12673000e-09 9.47907926e-07 9.31211553e-07 2.71092061e-06]\n",
      " [2.58722553e-05 6.53578927e-04 2.79608345e-04 5.60575708e-04]\n",
      " [2.84209783e-04 1.95564188e-04 5.62043440e-04 2.93135573e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "for ep in range(tot_eps):\n",
    "    # reset variables at start of new episode\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    reward = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        state_new, reward, done, _ = env.step(action)\n",
    "        qtable[state, action] = qtable[state, action] + lr * (reward + discount * np.max(qtable[state_new, :]) - qtable[state, action])\n",
    "        state = state_new\n",
    "print(\"Done!\")\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mm8oigYjzFTd"
   },
   "source": [
    "5. Evaluate how well your agent performs\n",
    "* Render output of one episode\n",
    "* Give an average episode return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "id": "wiXJPDzauAvV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.4\n",
      "[0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "for ep in range(tot_test_eps):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    tot_rewards = 0\n",
    "    while not done:\n",
    "        action = np.argmax(qtable[state, :])\n",
    "        state_new, reward, done, info = env.step(action)\n",
    "        tot_rewards += reward\n",
    "        state = state_new\n",
    "    rewards.append(tot_rewards)\n",
    "env.close()\n",
    "print (\"Score over time: \" +  str(sum(rewards)/tot_test_eps))\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.05611214e-03 1.64281208e-03 1.39869813e-03 1.39673710e-03]\n",
      " [1.00563496e-03 5.67034456e-07 2.88536652e-06 3.07583735e-05]\n",
      " [6.55521910e-05 0.00000000e+00 6.59519855e-09 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [7.02053403e-03 1.44890168e-03 1.44697200e-03 9.53765272e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.64752434e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [8.74426505e-04 1.19054506e-02 2.24297922e-03 3.14711184e-03]\n",
      " [3.31422364e-02 3.25140304e-03 4.13166005e-03 1.31475970e-03]\n",
      " [4.76239643e-02 4.69001990e-03 1.30624024e-03 1.31344633e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.49444366e-03 1.00557434e-01 3.31558901e-03 4.29033329e-03]\n",
      " [6.80956011e-03 1.38539825e-02 4.98670923e-03 2.85517985e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0yEuu22vVDK"
   },
   "source": [
    "# Subproject 2\n",
    "\n",
    "Solve [MoonLander-v2](https://gym.openai.com/envs/LunarLander-v2/) using DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWzbaAl3zlde"
   },
   "source": [
    "**1. Import Necessary Packages:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apijQg5Izmms",
    "outputId": "3c4e6d29-5f06-40fe-8047-900366ee662b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement swig (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for swig\u001b[0m\n",
      "Collecting box2d-py\n",
      "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
      "\u001b[K     |████████████████████████████████| 448 kB 4.4 MB/s \n",
      "\u001b[?25hInstalling collected packages: box2d-py\n",
      "Successfully installed box2d-py-2.3.8\n"
     ]
    }
   ],
   "source": [
    "!pip install box2d-py\n",
    "#Imports\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.optimizers import Adam\n",
    "import random\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBMyGxsqzwCd"
   },
   "source": [
    "**2. Instantiate the Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9dsd3yAz7Jp",
    "outputId": "1d6fe8a5-9d2d-4c02-ff70-2523a3d6c12a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (8,)\n",
      "Number of Actions:  4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of Actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Paf8yGHfz--m"
   },
   "source": [
    "**3. Implement and instantiate the agent**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgmFtepK0G1d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pw_aBoa40KkO"
   },
   "source": [
    "**4. Train the agent with DQN**\n",
    "\n",
    "4.1 Show the episode return plot\n",
    "  \n",
    "  - Is the agent learning to solve the task?\n",
    "\n",
    "4.2 Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsG2JUqF0N_B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSVgVSMp0OOV"
   },
   "source": [
    "**5. Load the model from the disk and run it in a loop**\n",
    "- Hint: if you want to see the agent laning the Moon Lander, type `env.render()` after the `env.step()`.\n",
    "- Do to Colab not cooperating with the Gym rendering, you might want to download the trained model and run this loop on you computer to visualise the behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQz2t49p1JHG"
   },
   "source": [
    "**Helper functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jhtc8jF1XB2"
   },
   "source": [
    "Save rendered images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_JgokDf1L0E"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "images = []\n",
    "images.append(img)\n",
    "img = model.env.render(mode='rgb_array')\n",
    "\n",
    "imageio.mimwrite('./moonlander.gif',\n",
    "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
    "                fps=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBRNoQ4X1heu"
   },
   "source": [
    "Display saved .gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1J7b1mCL1km8"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "gifPath = Path(\"./moonlander.gif\")\n",
    "# Display GIF in Jupyter, CoLab, IPython\n",
    "with open(gifPath,'rb') as f:\n",
    "    display.Image(data=f.read(), format='png')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of PROJECT_reinforcement_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
