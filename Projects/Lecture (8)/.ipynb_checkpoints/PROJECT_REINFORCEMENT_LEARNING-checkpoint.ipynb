{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGzC3uqmuKZB"
   },
   "source": [
    "# In this project we will solve two simple environments using a Q-table and a Neural Network (Deep Q-learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYeKUsX8uXSF"
   },
   "source": [
    "# Subproject 1\n",
    "\n",
    "Solve [`FrozenLake8x8-v0`](https://gym.openai.com/envs/FrozenLake8x8-v0/) using a Q-table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGAOGNSWyncb"
   },
   "source": [
    "1. Import Necessary Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7KHXZDxys6J"
   },
   "source": [
    "\n",
    "2. Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake8x8-v0\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMs2BVFZywAJ"
   },
   "source": [
    "3. Set up the QTable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions:  4\n",
      "States:  64\n",
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "print(\"Actions: \", action_size)\n",
    "state_size = env.observation_space.n\n",
    "print(\"States: \", state_size)\n",
    "obs = env.reset()\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHuDteJVy2_C"
   },
   "source": [
    "4. The Q-Learning algorithm training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "tot_eps = 100000\n",
    "tot_test_eps = 20\n",
    "\n",
    "lr = 0.01\n",
    "discount = 0.97\n",
    "\n",
    "epsilon = 1\n",
    "epsilon_max = 0.9\n",
    "epsilon_min = 0.01\n",
    "decay = 0.01\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "print(qtable.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100000 of 100000) |################| Elapsed Time: 0:02:38 Time:  0:02:38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import progressbar\n",
    "for ep in progressbar.progressbar(range(tot_eps)):\n",
    "    # reset variables at start of new episode\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    reward = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        state_new, reward, done, _ = env.step(action)\n",
    "        qtable[state, action] = qtable[state, action] + lr * (reward + discount * np.max(qtable[state_new, :]) - qtable[state, action])\n",
    "        state = state_new\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mm8oigYjzFTd"
   },
   "source": [
    "5. Evaluate how well your agent performs\n",
    "* Render output of one episode\n",
    "* Give an average episode return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "wiXJPDzauAvV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (20 of 20) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful episodes: [0, 4, 5, 6, 7, 8, 11, 12, 14, 15, 16, 17, 18, 19]\n",
      "/////////////////////////////////////////////////////\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "F\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Up)\n",
      "SFFFFFFF\n",
      "FF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0mFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0mFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0mFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFF\u001b[41mF\u001b[0mFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFFF\u001b[41mF\u001b[0mFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFFF\u001b[41mF\u001b[0mFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFFFF\u001b[41mF\u001b[0mF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFFFF\u001b[41mF\u001b[0mF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFFFFF\u001b[41mF\u001b[0m\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFFFFF\u001b[41mF\u001b[0m\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFF\u001b[41mF\u001b[0m\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFF\u001b[41mF\u001b[0m\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHF\u001b[41mF\u001b[0m\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFF\u001b[41mF\u001b[0m\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHF\u001b[41mF\u001b[0m\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFF\u001b[41mF\u001b[0m\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFF\u001b[41mF\u001b[0m\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFF\u001b[41mF\u001b[0m\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFH\u001b[41mF\u001b[0m\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFF\u001b[41mF\u001b[0m\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFF\u001b[41mF\u001b[0m\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFH\u001b[41mF\u001b[0m\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFH\u001b[41mF\u001b[0m\n",
      "FFFHFFFG\n",
      "\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n",
      "\n",
      "/////////////////////////////////////////////////////\n",
      "Score over time: 0.7\n",
      "[1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "hist = [[] for _ in range(tot_test_eps)]\n",
    "for ep in progressbar.progressbar(range(tot_test_eps)):        \n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    tot_rewards = 0\n",
    "    hist[ep].append(env.render(mode='ansi'))\n",
    "    while not done:\n",
    "        action = np.argmax(qtable[state, :])\n",
    "        state_new, reward, done, info = env.step(action)\n",
    "        tot_rewards += reward\n",
    "        state = state_new\n",
    "        hist[ep].append(env.render(mode='ansi'))\n",
    "    rewards.append(tot_rewards)\n",
    "env.close()\n",
    "success = np.argwhere(rewards == np.amax(rewards)).flatten().tolist()\n",
    "print(f\"Successful episodes: {success}\")\n",
    "print(\"/////////////////////////////////////////////////////\")\n",
    "all_steps = list(map(lambda x : len(x), hist))\n",
    "for s in hist[np.argmin(all_steps)]:\n",
    "    print(s)\n",
    "print(\"/////////////////////////////////////////////////////\")\n",
    "print (\"Score over time: \" +  str(sum(rewards)/tot_test_eps))\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0yEuu22vVDK"
   },
   "source": [
    "# Subproject 2\n",
    "\n",
    "Solve [MoonLander-v2](https://gym.openai.com/envs/LunarLander-v2/) using DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWzbaAl3zlde"
   },
   "source": [
    "**1. Import Necessary Packages:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apijQg5Izmms",
    "outputId": "3c4e6d29-5f06-40fe-8047-900366ee662b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: box2d-py in c:\\anaconda3\\lib\\site-packages (2.3.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install box2d-py\n",
    "#Imports\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import random\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBMyGxsqzwCd"
   },
   "source": [
    "**2. Instantiate the Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9dsd3yAz7Jp",
    "outputId": "1d6fe8a5-9d2d-4c02-ff70-2523a3d6c12a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (8,)\n",
      "Number of Actions:  4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of Actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Paf8yGHfz--m"
   },
   "source": [
    "**3. Implement and instantiate the agent**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "kgmFtepK0G1d"
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(\n",
    "        self, states, actions, lr, batch_size, # neural network stuff\n",
    "        gamma, epsilon, epsilon_min, epsilon_decay # coefficients\n",
    "    ):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque([], maxlen=100000)\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        self.loss = []\n",
    "        \n",
    "    def build_model(self, units_fc1 = 128, units_fc2 = 128):\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Dense(units_fc1, input_dim = self.states, activation='relu')) \n",
    "        model.add(keras.layers.Dense(units_fc2, activation='relu')) \n",
    "        model.add(keras.layers.Dense(self.actions, activation='linear'))\n",
    "        model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam(learning_rate = self.lr)) \n",
    "        return model\n",
    "\n",
    "    def action(self, state):\n",
    "        if np.random.rand() <= self.epsilon: # should we explore?\n",
    "            return random.randrange(self.actions) # why not\n",
    "        return self.exploit(state) # nah, let's exploit\n",
    "\n",
    "    def exploit(self, state):\n",
    "        vals = self.model.predict(state)\n",
    "        return np.argmax(vals[0])\n",
    "\n",
    "    def store(self, state, action, reward, nstate, done):\n",
    "        self.memory.append((state, action, reward, nstate, done))\n",
    "\n",
    "    def experience_replay(self):\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "        x = []\n",
    "        y = []\n",
    "        n = np.array(mini_batch, dtype='object')\n",
    "        st = np.zeros((0, self.states))\n",
    "        nst = np.zeros((0, self.states))\n",
    "        for i in range(len(n)):\n",
    "            st = np.append( st, n[i, 0], axis=0)\n",
    "            nst = np.append( nst, n[i, 3], axis=0)\n",
    "        st_predict = self.model.predict(st)\n",
    "        nst_predict = self.model.predict(nst)\n",
    "        index = 0\n",
    "        for state, action, reward, next_state, done in mini_batch:\n",
    "            x.append(state)\n",
    "            predict_actions = nst_predict[index]\n",
    "            if done == True:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + self.gamma * np.amax(predict_actions)\n",
    "            target_f = st_predict[index]\n",
    "            target_f[action] = target\n",
    "            y.append(target_f)\n",
    "            index += 1\n",
    "        x_reshape = np.array(x).reshape(self.batch_size, self.states)\n",
    "        y_reshape = np.array(y)\n",
    "        epoch_count = 1\n",
    "        hist = self.model.fit(x_reshape, y_reshape, epochs = epoch_count, verbose=0)\n",
    "        for i in range(epoch_count):\n",
    "            self.loss.append( hist.history['loss'][i] )\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pw_aBoa40KkO"
   },
   "source": [
    "**4. Train the agent with DQN**\n",
    "\n",
    "4.1 Show the episode return plot\n",
    "  \n",
    "  - Is the agent learning to solve the task?\n",
    "\n",
    "4.2 Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wsG2JUqF0N_B",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1000, score: -109.43353595470799, e: 0.8690529955452602, terminal: True\n",
      "episode: 1/1000, score: -484.4234492529217, e: 0.4222502236424958, terminal: True\n",
      "episode: 2/1000, score: -688.1426636461046, e: 0.20826882814336947, terminal: True\n",
      "episode: 3/1000, score: -318.624147090209, e: 0.12743425563174798, terminal: True\n",
      "episode: 4/1000, score: -213.1794386533012, e: 0.09480864735409487, terminal: True\n",
      "episode: 5/1000, score: -137.50795396944375, e: 0.056575091797066025, terminal: True\n",
      "episode: 6/1000, score: -425.9270786199924, e: 0.03750748018035199, terminal: True\n",
      "episode: 7/1000, score: -395.48484209311874, e: 0.02260731802731653, terminal: True\n",
      "episode: 8/1000, score: -388.22018790318026, e: 0.01335588042198471, terminal: True\n",
      "episode: 9/1000, score: -1111.4637589305362, e: 0.00196829127312784, terminal: True\n",
      "episode: 10/1000, score: -426.1804389137089, e: 0.0009954703940636294, terminal: False\n",
      "episode: 11/1000, score: -110.85764260527773, e: 0.0009954703940636294, terminal: False\n",
      "episode: 12/1000, score: -244.86066252240266, e: 0.0009954703940636294, terminal: True\n",
      "episode: 13/1000, score: -171.98009625933943, e: 0.0009954703940636294, terminal: False\n",
      "episode: 14/1000, score: -168.43458423396999, e: 0.0009954703940636294, terminal: True\n",
      "episode: 15/1000, score: -73.76815223696309, e: 0.0009954703940636294, terminal: False\n",
      "episode: 16/1000, score: -108.654755222707, e: 0.0009954703940636294, terminal: False\n",
      "episode: 17/1000, score: -183.08625009315, e: 0.0009954703940636294, terminal: True\n",
      "episode: 18/1000, score: -155.4211319478504, e: 0.0009954703940636294, terminal: True\n",
      "episode: 19/1000, score: -212.70934778766525, e: 0.0009954703940636294, terminal: True\n",
      "episode: 20/1000, score: -245.64810230757084, e: 0.0009954703940636294, terminal: True\n",
      "episode: 21/1000, score: -163.48705494485725, e: 0.0009954703940636294, terminal: True\n",
      "episode: 22/1000, score: -198.55839704845945, e: 0.0009954703940636294, terminal: True\n",
      "episode: 23/1000, score: -184.03527277251618, e: 0.0009954703940636294, terminal: True\n",
      "episode: 24/1000, score: -204.5789211014074, e: 0.0009954703940636294, terminal: True\n",
      "episode: 25/1000, score: -168.7170642970185, e: 0.0009954703940636294, terminal: True\n",
      "episode: 26/1000, score: -146.89510780013705, e: 0.0009954703940636294, terminal: True\n",
      "episode: 27/1000, score: -241.21797495411855, e: 0.0009954703940636294, terminal: True\n",
      "episode: 28/1000, score: -206.78779232878594, e: 0.0009954703940636294, terminal: False\n",
      "episode: 29/1000, score: -213.56821657118715, e: 0.0009954703940636294, terminal: True\n",
      "episode: 30/1000, score: -151.8355944351294, e: 0.0009954703940636294, terminal: True\n",
      "episode: 31/1000, score: -175.06162294670565, e: 0.0009954703940636294, terminal: True\n",
      "episode: 32/1000, score: -160.30497918075366, e: 0.0009954703940636294, terminal: True\n",
      "episode: 33/1000, score: -186.31973651073233, e: 0.0009954703940636294, terminal: False\n",
      "episode: 34/1000, score: -154.61783275918472, e: 0.0009954703940636294, terminal: False\n",
      "episode: 35/1000, score: -162.4164168918201, e: 0.0009954703940636294, terminal: True\n",
      "episode: 36/1000, score: -201.991875302928, e: 0.0009954703940636294, terminal: True\n",
      "episode: 37/1000, score: -146.65569764044233, e: 0.0009954703940636294, terminal: True\n",
      "episode: 38/1000, score: -258.5491473873136, e: 0.0009954703940636294, terminal: True\n",
      "episode: 39/1000, score: -215.04042582872293, e: 0.0009954703940636294, terminal: True\n",
      "episode: 40/1000, score: -220.18656402912922, e: 0.0009954703940636294, terminal: True\n",
      "episode: 41/1000, score: -205.94322132799232, e: 0.0009954703940636294, terminal: True\n",
      "episode: 42/1000, score: -180.59659776844032, e: 0.0009954703940636294, terminal: True\n",
      "episode: 43/1000, score: -223.93863649249943, e: 0.0009954703940636294, terminal: True\n",
      "episode: 44/1000, score: -225.9556336964911, e: 0.0009954703940636294, terminal: True\n",
      "episode: 45/1000, score: -257.97969461199483, e: 0.0009954703940636294, terminal: True\n",
      "episode: 46/1000, score: -258.4212801953714, e: 0.0009954703940636294, terminal: True\n",
      "episode: 47/1000, score: -241.60637724708732, e: 0.0009954703940636294, terminal: True\n",
      "episode: 48/1000, score: -237.80790088685185, e: 0.0009954703940636294, terminal: True\n",
      "episode: 49/1000, score: -261.27558384845986, e: 0.0009954703940636294, terminal: True\n",
      "episode: 50/1000, score: -225.31121954722158, e: 0.0009954703940636294, terminal: True\n",
      "episode: 51/1000, score: -177.37767365222993, e: 0.0009954703940636294, terminal: True\n",
      "episode: 52/1000, score: -267.826645563796, e: 0.0009954703940636294, terminal: True\n",
      "episode: 53/1000, score: -262.5725814234415, e: 0.0009954703940636294, terminal: True\n",
      "episode: 54/1000, score: -289.34166953033326, e: 0.0009954703940636294, terminal: True\n",
      "episode: 55/1000, score: -236.80364452829193, e: 0.0009954703940636294, terminal: True\n",
      "episode: 56/1000, score: -258.93320944805737, e: 0.0009954703940636294, terminal: True\n",
      "episode: 57/1000, score: -180.13053806221953, e: 0.0009954703940636294, terminal: True\n",
      "episode: 58/1000, score: -249.0280581843051, e: 0.0009954703940636294, terminal: True\n",
      "episode: 59/1000, score: -314.91311572489144, e: 0.0009954703940636294, terminal: True\n",
      "episode: 60/1000, score: -228.10366984962204, e: 0.0009954703940636294, terminal: False\n",
      "episode: 61/1000, score: -94.98622860162651, e: 0.0009954703940636294, terminal: False\n",
      "episode: 62/1000, score: -299.32161385157895, e: 0.0009954703940636294, terminal: True\n",
      "episode: 63/1000, score: -160.48454903792623, e: 0.0009954703940636294, terminal: True\n",
      "episode: 64/1000, score: -97.18451226997077, e: 0.0009954703940636294, terminal: False\n",
      "episode: 65/1000, score: -171.2256496788939, e: 0.0009954703940636294, terminal: False\n",
      "episode: 66/1000, score: -184.88980349320036, e: 0.0009954703940636294, terminal: True\n",
      "episode: 67/1000, score: -157.76902378535883, e: 0.0009954703940636294, terminal: True\n",
      "episode: 68/1000, score: -130.3267808127556, e: 0.0009954703940636294, terminal: True\n",
      "episode: 69/1000, score: -240.3101567693786, e: 0.0009954703940636294, terminal: True\n",
      "episode: 70/1000, score: -230.64907287292846, e: 0.0009954703940636294, terminal: False\n",
      "episode: 71/1000, score: -179.991947270246, e: 0.0009954703940636294, terminal: False\n",
      "episode: 72/1000, score: -301.8406468610241, e: 0.0009954703940636294, terminal: True\n",
      "episode: 73/1000, score: -122.07063963981385, e: 0.0009954703940636294, terminal: True\n",
      "episode: 74/1000, score: -207.3786318827923, e: 0.0009954703940636294, terminal: False\n",
      "episode: 75/1000, score: -240.17515153604867, e: 0.0009954703940636294, terminal: True\n",
      "episode: 76/1000, score: -160.57165499893327, e: 0.0009954703940636294, terminal: False\n",
      "episode: 77/1000, score: -145.33919689112113, e: 0.0009954703940636294, terminal: True\n",
      "episode: 78/1000, score: -113.02048176159859, e: 0.0009954703940636294, terminal: False\n",
      "episode: 79/1000, score: -160.59721189753083, e: 0.0009954703940636294, terminal: False\n",
      "episode: 80/1000, score: -197.60664738196334, e: 0.0009954703940636294, terminal: True\n",
      "episode: 81/1000, score: -180.95150037280033, e: 0.0009954703940636294, terminal: True\n",
      "episode: 82/1000, score: -180.1886347422959, e: 0.0009954703940636294, terminal: True\n",
      "episode: 83/1000, score: -240.84380889260567, e: 0.0009954703940636294, terminal: True\n",
      "episode: 84/1000, score: -148.02896408667795, e: 0.0009954703940636294, terminal: True\n",
      "episode: 85/1000, score: -218.313061635743, e: 0.0009954703940636294, terminal: True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8444/2543252050.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtot_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mnstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mnstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8444/3091032081.py\u001b[0m in \u001b[0;36maction\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# should we explore?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# why not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexploit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# nah, let's exploit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexploit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8444/3091032081.py\u001b[0m in \u001b[0;36mexploit\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexploit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1721\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1722\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1723\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Single epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1724\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1725\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1197\u001b[0m     \u001b[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1199\u001b[1;33m       \u001b[0mdata_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1200\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    694\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    717\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m--> 719\u001b[1;33m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m       \u001b[1;31m# Delete the resource when this object is deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3117\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3118\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3119\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   3120\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0;32m   3121\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import progressbar\n",
    "episodes = 1000\n",
    "dqn = DQN(env.observation_space.shape[0], env.action_space.n, 0.0001, 64, 0.99, 1, 0.001, 0.995)\n",
    "rewards = [] #Store rewards for graphing\n",
    "epsilons = [] # Store the Explore/Exploit\n",
    "test_episodes = 0\n",
    "frames = 900\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, dqn.states])\n",
    "    tot_rewards = 0\n",
    "    for frame in range(frames):\n",
    "        action = dqn.action(state)\n",
    "        nstate, reward, done, _ = env.step(action)\n",
    "        nstate = np.reshape(nstate, [1, dqn.states])\n",
    "        tot_rewards += reward\n",
    "        dqn.store(state, action, reward, nstate, done)\n",
    "        state = nstate\n",
    "        if done or frame == frames - 1:\n",
    "            rewards.append(tot_rewards)\n",
    "            epsilons.append(dqn.epsilon)\n",
    "            print(f\"episode: {e}/{episodes}, score: {tot_rewards}, e: {dqn.epsilon}, terminal: {done}\")\n",
    "            break\n",
    "        if len(dqn.memory) > dqn.batch_size:\n",
    "            dqn.experience_replay()\n",
    "    if len(rewards) > 5 and np.average(rewards[-5:]) > 200:\n",
    "        test_episodes = episodes - e\n",
    "        train_end = e\n",
    "        break\n",
    "dqn.model.save(dqn.model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e_test in range(test_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, nS])\n",
    "    tot_rewards = 0\n",
    "    frames = 900\n",
    "    for frame in range(frames):\n",
    "        action = dqn.exploit(state)\n",
    "        nstate, reward, done, _ = env.step(action)\n",
    "        nstate = np.reshape( nstate, [1, nS])\n",
    "        tot_rewards += reward\n",
    "        state = nstate\n",
    "        if done or frame == frame - 1: \n",
    "            rewards.append(tot_rewards)\n",
    "            epsilons.append(0)\n",
    "            print(\"episode: {}/{}, score: {}, e: {}\"\n",
    "                  .format(e_test, TEST_Episodes, tot_rewards, 0))\n",
    "            break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_average = np.convolve(rewards, np.ones(100)/100)\n",
    "plt.plot(rewards)\n",
    "plt.plot(rolling_average, color='black')\n",
    "plt.axhline(y=200, color='r', linestyle='-')\n",
    "eps_graph = [200*x for x in epsilons]\n",
    "plt.plot(eps_graph, color='g', linestyle='-')\n",
    "plt.axvline(x = train_end, color='y', linestyle='-')\n",
    "plt.xlim( (0, episodes) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSVgVSMp0OOV"
   },
   "source": [
    "**5. Load the model from the disk and run it in a loop**\n",
    "- Hint: if you want to see the agent laning the Moon Lander, type `env.render()` after the `env.step()`.\n",
    "- Do to Colab not cooperating with the Gym rendering, you might want to download the trained model and run this loop on you computer to visualise the behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(dqn.model.name)\n",
    "while True:\n",
    "    state = env.reset()\n",
    "    vals = model.predict(state)\n",
    "    action = np.argmax(vals[0])\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQz2t49p1JHG"
   },
   "source": [
    "**Helper functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jhtc8jF1XB2"
   },
   "source": [
    "Save rendered images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_JgokDf1L0E"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "images = []\n",
    "images.append(img)\n",
    "img = model.env.render(mode='rgb_array')\n",
    "\n",
    "imageio.mimwrite('./moonlander.gif',\n",
    "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
    "                fps=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBRNoQ4X1heu"
   },
   "source": [
    "Display saved .gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1J7b1mCL1km8"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "gifPath = Path(\"./moonlander.gif\")\n",
    "# Display GIF in Jupyter, CoLab, IPython\n",
    "with open(gifPath,'rb') as f:\n",
    "    display.Image(data=f.read(), format='png')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of PROJECT_reinforcement_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
